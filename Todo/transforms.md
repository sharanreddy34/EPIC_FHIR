PRODUCTION-GRADE BRONZE âœ SILVER âœ GOLD PLAN
(for the epic-fhir-integration package running in Palantir Foundry)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
FOUNDATIONAL PRINCIPLES
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Medallion architecture â€“ each layer has a single purpose:
Bronze: immutable raw JSON bundles exactly as returned by Epic.
Silver: 1 row â‰ˆ 1 FHIR resource, cleaned & flattened; schema is stable.
Gold: business-ready tables & KPI datasets (aggregations, joins, denormalised views).
Every transform is idempotent, incremental, well-typed, logged, unit-tested.
Dataset paths are contracts; changing them requires explicit migration steps.
All credentials live in Foundry Secrets; NO plainâ€text keys in code or logs.
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DIRECTORY & MODULE LAYOUT
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
epic-fhir-integration/
â”‚
â”œâ”€ config/ â‡¦ YAMLs only
â”‚ â”œâ”€ api_config.yaml (FHIR base URL, retry settings â€¦)
â”‚ â”œâ”€ resources_config.yaml (enabled resources, incremental params â€¦)
â”‚ â””â”€ gold_views.yaml (high-level definitions of Gold views)
â”‚
â”œâ”€ pipelines/ â‡¦ Spark transforms
â”‚ â”œâ”€ 00_fetch_token.py
â”‚ â”œâ”€ 01_extract_resources.py
â”‚ â”œâ”€ 02_transform_load.py
â”‚ â”œâ”€ 03_gold/
â”‚ â”‚ â”œâ”€ patient_summary.py
â”‚ â”‚ â”œâ”€ encounter_summary.py
â”‚ â”‚ â””â”€ â€¦ (one file per gold mart)
â”‚ â”œâ”€ 04_workflow.py (orchestrator)
â”‚ â””â”€ 05_monitoring.py
â”‚
â”œâ”€ lib/ â‡¦ pure Python helpers
â”‚ â”œâ”€ fhir_client.py (all HTTP logic)
â”‚ â”œâ”€ auth.py (token mgmt helpers)
â”‚ â””â”€ transforms/
â”‚ â”œâ”€ common.py (explode_bundle, json schema utils)
â”‚ â””â”€ resource_specific/
â”‚ â”œâ”€ patient.py
â”‚ â”œâ”€ observation.py
â”‚ â””â”€ â€¦
â”‚
â””â”€ tests/ â‡¦ pytest unit + integration tests
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DATASET PATH CONTRACTS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
/config/ api_config resources_config
/secrets/ epic_token (Î” only via 00)
/control/ fhir_cursors workflow_status
/bronze/ fhir_raw/<ResourceType>/.json
/silver/ fhir_normalized/<lowercase resource type>
/gold/ patient_summary encounter_summary â€¦
/metrics/ transform_metrics
/monitoring/ pipeline_metrics
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PIPELINE STEPS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”¹ 00_fetch_token.py (Profile: FHIR_TOKEN_REFRESH)
Reads /config/api_config.yaml
POST client-cred to token endpoint
Writes single-row Delta to /secrets/epic_token
Schema: access_token (string), expires_at (timestamp)
ğŸ”¹ 01_extract_resources.py (Profile: FHIR_RESOURCE_EXTRACTION)
Inputs:
/config/api_config.yaml, /config/resources_config.yaml
/secrets/epic_token (for current token)
/control/fhir_cursors (optional) (last updated stamps)
Output: /bronze/fhir_raw
Logic:
Loop over enabled resources in config (or CLI param).
For each, call FHIRClient.search_resource() with incremental params.
For each bundle page write one JSON file â‡’
{resource_type}/{yyyyMMdd_HHmmss}{page}.json
4. Update cursors dataset atomically (resource, max_lastUpdated).
ğŸ”¹ 02_transform_load.py (Profile: FHIR_RESOURCE_TRANSFORM)
Inputs: /bronze/fhir_raw, resources_config.yaml
Outputs:
/silver/fhir_normalized (dataset, partitioned by resource_type)
/metrics/transform_metrics (one row per run|resource)
Logic:
For each resource file glob read JSON with Spark.
Flatten with explode_bundle() + resource_specific/.py mapper.
Enforce explicit StructType â†’ write in append mode, mergeSchema false.
Capture counts + runtime in transform_metrics.
ğŸ”¹ 03_gold/â€¦ (multiple transforms, one per business mart)
Example: patient_summary.py (Profile: FHIR_GOLD_PATIENT_SUMMARY)
Inputs: /silver/fhir_normalized/patient plus optional others
Output: /gold/patient_summary
Aggregate metrics (encounter counts, last visit date, diagnoses, etc.)
All joins by patient.id, left joins to keep all patients.
ğŸ”¹ 04_workflow.py (Profile: FHIR_PIPELINE_ORCHESTRATOR)
Inputs: resources_config, api_config, epic_token, cursors, metrics
Output: /control/workflow_status
Determines which resources need refresh; triggers downstream steps when run via Foundry Workflow UI.
Writes status row (JSON string) for observability.
ğŸ”¹ 05_monitoring.py (Profile: FHIR_PIPELINE_MONITORING)
Combines token life-cycle, extraction stats, transform stats, patient counts into one wide table in /monitoring/pipeline_metrics.
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
FUNCTION / TRANSFORM YAMLs
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Each .py above needs a matching YAML (e.g. pipelines/00_fetch_token.yml).
Skeleton (example for 00_fetch_token):
Apply to .gitignore
4h
Copy this pattern for the other transforms with correct paths, optional inputs, and profiles.
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
WORKFLOW DEFINITION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Use Foundry Workflow UI OR a declarative YAML (workflow_pipeline.yml):
refresh-token (00) â†’ always first
extract-resources (01) â†’ after token
transform-resources (02) â†’ after extract
gold-patient-summary (03a) â†’ after transform
gold-<other> (03b â€¦)
monitoring (05) â†’ after gold
orchestrator (04) can run async to produce status but usually triggers others in UI.
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
INCREMENTAL STRATEGY
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Cursor dataset /control/fhir_cursors (resource_type, last_updated) keeps state.
01_extract_resources reads cursor; sends _lastUpdated=gt<ts> param.
After successful write it upserts new max timestamp.
02_transform_load can filter by input file modification time if needed (fast path).
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DATA QUALITY & SCHEMA EVOLUTION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Each Silver table kept under strict schema control (StructType JSON).
Breaking changes handled by versioned columns not dropping old ones until migration.
Transform writes with mergeSchema = false (fail-fast).
Row-level QA metrics written to /metrics/transform_metrics.
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
LOGGING & ALERTING
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Use standard logging with context (resource_type, run_id).
05_monitoring aggregates => choose thresholds, attach to Foundry Watchdog alerts.
Token expiry < 12 h â†’ WARNING; < 1 h â†’ CRITICAL alert.
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TESTING & CI/CD
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
tests/unit/ â€“ pytest covering lib/ modules (mock requests, chispa for DataFrames).
tests/integration/ â€“ run Spark locally with small sample JSON bundles.
GitHub Actions: lint â†’ unit â†’ package build â†’ optional integration.
On merge to main: push to Foundry code repo via Foundryâ€™s Git inbound.
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ROLLOUT CHECKLIST
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â˜ Secrets (EPIC_CLIENT_ID, EPIC_CLIENT_SECRET) created in Foundry
â˜ Datasets paths pre-provisioned with correct ACLs
â˜ All YAMLs committed & pushed â†’ Foundry auto-compiles
â˜ Run workflow in DEV stack with one resource enabled
â˜ Verify Bronze JSON, Silver counts, Gold aggregates, Monitoring rows
â˜ Enable remaining resources, schedule nightly full workflow
â˜ Set retention policies (Bronze = 90 days, Silver = 1 year, Gold = forever)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
With this plan, a less-capable LLM (or a junior engineer) can:
Scaffold the transforms exactly as described.
Copy the skeleton YAML, adjust paths & resources.
Rely on existing helper modules in lib/ for HTTP + parsing.
Deploy & iterate safely inside Foundry.